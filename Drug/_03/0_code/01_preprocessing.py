import numpy as np
import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors
from rdkit.ML.Descriptors import MoleculeDescriptors
from rdkit.Chem import rdFingerprintGenerator
import os
import random

# Seed ê³ ì •
r = random.randint(1,1000)
random.seed(r)
np.random.seed(r)

# ğŸ” ë³€ìˆ˜ ì´ˆê¸°í™” (ì¬ì‹¤í–‰ ì•ˆì „)
train_mols, test_mols = [], []
train_descriptor_df, test_descriptor_df = None, None
train_fingerprint_df, test_fingerprint_df = None, None

# ğŸ“‚ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
train_csv = pd.read_csv('./Drug/train.csv', index_col=0)
test_csv = pd.read_csv('./Drug/test.csv', index_col=0)

# ğŸ“¦ SMILES â†’ Mol ê°ì²´ ë³€í™˜
train_mols = [Chem.MolFromSmiles(smi) for smi in train_csv['Canonical_Smiles']]
test_mols = [Chem.MolFromSmiles(smi) for smi in test_csv['Canonical_Smiles']]

# â— SMILES ì˜¤ë¥˜ í™•ì¸
invalid_train = sum([mol is None for mol in train_mols])
invalid_test = sum([mol is None for mol in test_mols])

# ğŸ§ª ë””ìŠ¤í¬ë¦½í„° ê³„ì‚°ê¸°
descriptor_names = [desc[0] for desc in Descriptors._descList]
calc = MoleculeDescriptors.MolecularDescriptorCalculator(descriptor_names)

def calc_descriptors(mols):
    return [calc.CalcDescriptors(mol) if mol is not None else [None]*len(descriptor_names) for mol in mols]

# ğŸ§¬ ìµœì‹  Morgan Fingerprint ìƒì„±ê¸°
morgan_gen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)

def get_fingerprint(mol):
    if mol is None:
        return [0]*2048
    fp = morgan_gen.GetFingerprint(mol)
    return [int(b) for b in fp.ToBitString()]

def calc_fingerprints(mols):
    return [get_fingerprint(mol) for mol in mols]

# ğŸ§® ë””ìŠ¤í¬ë¦½í„° + í•‘ê±°í”„ë¦°íŠ¸ ê³„ì‚°
train_descriptor_df = pd.DataFrame(calc_descriptors(train_mols), columns=descriptor_names)
test_descriptor_df = pd.DataFrame(calc_descriptors(test_mols), columns=descriptor_names)

train_fingerprint_df = pd.DataFrame(calc_fingerprints(train_mols), columns=[f'FP_{i}' for i in range(2048)])
test_fingerprint_df = pd.DataFrame(calc_fingerprints(test_mols), columns=[f'FP_{i}' for i in range(2048)])



# ìŠ¤ë§ˆì¼ì¦ˆ ëª©ë¡ ë¶„ë¦¬
train_smiles = train_csv['Canonical_Smiles']
test_smiles = test_csv['Canonical_Smiles']

# ğŸ§· ì¸ë±ìŠ¤ ì •ë ¬ í›„ ë³‘í•©
train = pd.concat([
    train_smiles.reset_index(drop=True).rename('Canonical_Smiles'),
    train_descriptor_df.reset_index(drop=True),
    train_fingerprint_df.reset_index(drop=True),
    train_csv[['Inhibition']].reset_index(drop=True)
], axis=1)

test = pd.concat([
    test_smiles.reset_index(drop=True).rename('Canonical_Smiles'),
    test_descriptor_df.reset_index(drop=True),
    test_fingerprint_df.reset_index(drop=True)
], axis=1)

# ğŸ“ˆ í™•ì¸
print("âœ… Final Train shape:", train.shape)
print("âœ… Final Test shape :", test.shape)

# ğŸ’¡ ìˆ˜ì¹˜í˜• / ë²”ì£¼í˜• ë¶„ë¦¬ ê¸°ì¤€
descriptor_cols = descriptor_names  # ìˆ˜ì¹˜í˜•
fingerprint_cols = [f'FP_{i}' for i in range(2048)]  # ë²”ì£¼í˜•
# RDKit ë””ìŠ¤í¬ë¦½í„°ì™€ Fingerprint ì—´ ëª©ë¡
descriptor_cols = descriptor_names
fingerprint_cols = [f'FP_{i}' for i in range(2048)]

# ìˆ˜ì¹˜í˜• í”¼ì²˜: floatì´ê±°ë‚˜ ìœ ë‹ˆí¬ ê°’ì´ ë§ìŒ
numeric_cols = []
categorical_cols = []

# ëª¨ë“  ì—´ ê²€ì‚¬
for col in descriptor_cols + fingerprint_cols:
    series = train[col]
    nunique = series.nunique()
    dtype = series.dtype

    if pd.api.types.is_float_dtype(dtype):
        numeric_cols.append(col)
    elif nunique <= 2:
        categorical_cols.append(col)
    elif nunique < 20:
        # ì›í•˜ë©´ ì—¬ê¸°ì„œë„ ë²”ì£¼í˜•ìœ¼ë¡œ ë„£ì„ ìˆ˜ ìˆìŒ
        categorical_cols.append(col)
    else:
        numeric_cols.append(col)

# SMILESì™€ Inhibitionì€ ë”°ë¡œ ì €ì¥
meta_cols = ['Canonical_Smiles']
if 'Inhibition' in train.columns:
    meta_cols.append('Inhibition')

# ë¶„ë¦¬í•´ì„œ ë¶™ì—¬ë‘ê¸°
train_numeric = pd.concat([train[meta_cols], train[numeric_cols]], axis=1)
train_categorical = pd.concat([train[meta_cols], train[categorical_cols]], axis=1)

test_numeric = pd.concat([test[['Canonical_Smiles']], test[numeric_cols]], axis=1)
test_categorical = pd.concat([test[['Canonical_Smiles']], test[categorical_cols]], axis=1)

train_numeric.to_csv('./Drug/_03/train_nu_siles.csv')
train_categorical.to_csv('./Drug/_03/train_ca_siles.csv')
test_numeric.to_csv('./Drug/_03/test_nu_siles.csv')
test_categorical.to_csv('./Drug/_03/test_ca_siles.csv')

exit()

# print(train_numeric.shape)          (1681, 111)
# print(test_numeric.shape)           (100, 110)
# print(train_categorical.shape)      (1681, 2158)
# print(test_categorical.shape)       (100, 2157)

# ë‘ ë°ì´í„°í”„ë ˆì„ì—ì„œ NaN ì—†ëŠ” ì—´ì„ ê³µí†µì ìœ¼ë¡œ ì¶”ì¶œ
train_nan_cols = train_numeric.columns[train_numeric.isna().any()].tolist()
test_nan_cols = test_numeric.columns[test_numeric.isna().any()].tolist()

# ê³µí†µì ìœ¼ë¡œ ì œê±°í•  ì—´
nan_cols_to_drop = list(set(train_nan_cols + test_nan_cols))

# ë™ì‹œì— ì œê±°
train_numeric = train_numeric.drop(columns=nan_cols_to_drop)
test_numeric = test_numeric.drop(columns=nan_cols_to_drop)

# NaNì´ ìˆëŠ” ì—´ í™•ì¸
def drop_nan_columns(df, df_name):
    nan_cols = df.columns[df.isna().any()].tolist()
    return df.drop(columns=nan_cols)

train_categorical = drop_nan_columns(train_categorical, "train_categorical")
test_categorical = drop_nan_columns(test_categorical, "test_categorical")

# print(train_numeric.shape)        #  (1681, 103)
# print(test_numeric.shape)         #  (100, 102)
# print(train_categorical.shape)    #  (1681, 2158)
# print(test_categorical.shape)     #  (100, 2157)


# feature_importance
print('feature_importance ì‹œì‘')
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier, XGBRegressor
import numpy as np

# ğŸ¯ ìˆ˜ì¹˜í˜•
x_nu = train_numeric.drop(['Canonical_Smiles', 'Inhibition'], axis=1)
y_nu = train_numeric['Inhibition']
test_nu = test_numeric.drop(['Canonical_Smiles'], axis=1)

x1_train, x1_test, y1_train, y1_test = train_test_split(
    x_nu, y_nu, random_state=r
)

model1 = XGBRegressor(random_state=r) 
model1.fit(x1_train, y1_train)
print("==========", model1.__class__.__name__, "==========")
print('R2 :', model1.score(x1_test, y1_test))

# ì¤‘ìš”ë„ ìƒìœ„ 50ê°œ ì„ íƒ
top50_nu_idx = np.argsort(model1.feature_importances_)[::-1][:50]
top50_nu_cols = x_nu.columns[top50_nu_idx]

# ğŸ¯ ë²”ì£¼í˜•
x_ca = train_categorical.drop(['Canonical_Smiles', 'Inhibition'], axis=1)
y_ca = train_categorical['Inhibition']
test_ca = test_categorical.drop(['Canonical_Smiles'], axis=1)

from sklearn.preprocessing import LabelEncoder
y_ca_rint = np.rint(y_ca).astype(int)
le = LabelEncoder()
y_ca_rint_enc = le.fit_transform(y_ca_rint)

x2_train, x2_test, y2_train, y2_test = train_test_split(
    x_ca, y_ca_rint_enc, random_state=r,
)

model2 = XGBClassifier(random_state=r) 
model2.fit(x2_train, y2_train)
print("==========", model2.__class__.__name__, "==========")
print('ACC :', model2.score(x2_test, y2_test))

# ì¤‘ìš”ë„ ìƒìœ„ 1000ê°œ ì„ íƒ
top1000_ca_idx = np.argsort(model2.feature_importances_)[::-1][:1000]
top1000_ca_cols = x_ca.columns[top1000_ca_idx]

# âœ… ì •ë ¬ ë° ì—´ ì œí•œ í›„ ë³‘í•©
train_nu_sorted = pd.concat([
    train_numeric[['Canonical_Smiles', 'Inhibition']].reset_index(drop=True),
    x_nu[top50_nu_cols].reset_index(drop=True)
], axis=1)

test_nu_sorted = pd.concat([
    test_numeric[['Canonical_Smiles']].reset_index(drop=True),
    test_nu[top50_nu_cols].reset_index(drop=True)
], axis=1)

train_ca_sorted = pd.concat([
    train_categorical[['Canonical_Smiles', 'Inhibition']].reset_index(drop=True),
    x_ca[top1000_ca_cols].reset_index(drop=True)
], axis=1)

test_ca_sorted = pd.concat([
    test_categorical[['Canonical_Smiles']].reset_index(drop=True),
    test_ca[top1000_ca_cols].reset_index(drop=True)
], axis=1)

# âœ… í™•ì¸
# print("ğŸ“ Shapes after top-k filtering")
# print("  train_numeric:", train_nu_sorted.shape)      #  train_numeric: (1681, 52)
# print("  test_numeric :", test_nu_sorted.shape)       #  test_numeric : (100, 51)
# print("  train_categorical:", train_ca_sorted.shape)  #  train_categorical: (1681, 1002)
# print("  test_categorical :", test_ca_sorted.shape)   #  test_categorical : (100, 1001)

# numeric ë°ì´í„° PCA
numeric_x = train_nu_sorted.drop(
    ['Canonical_Smiles', 'Inhibition'], axis=1
)
numeric_test = test_nu_sorted.drop(
    ['Canonical_Smiles'], axis=1
)
numeric_y = train_nu_sorted['Inhibition']

from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
numeric_x_scaled = ss.fit_transform(numeric_x)
numeric_test_scaled = ss.transform(numeric_test)

pca = PCA(n_components=35)
numeric_x_pca = pca.fit_transform(numeric_x_scaled)
numeric_test_pca = pca.transform(numeric_test_scaled)
# evr = np.cumsum(pca.explained_variance_ratio_)
# # print(evr)
# num = [0.999, 0.99, 0.95, 0.90]
# for i in num :
#     threshold = i            
#     compo_1 = np.argmax(evr>= threshold) +1 
#     print(f'{i} :',compo_1) 
# 0.999 : 44
# 0.99  : 35
# 0.95  : 27
# 0.9   : 21

pca_train = pd.concat([
    train_numeric[['Canonical_Smiles']].reset_index(drop=True),
    pd.DataFrame(numeric_x_pca, columns=[f'PC_{i+1}' for i in range(numeric_x_pca.shape[1])]),
    train_numeric[['Inhibition']].reset_index(drop=True)
], axis=1)

pca_test = pd.concat([
    test_numeric[['Canonical_Smiles']].reset_index(drop=True),
    pd.DataFrame(numeric_test_pca, columns=[f'PC_{i+1}' for i in range(numeric_test_pca.shape[1])])
], axis=1)

# Categorical ë°ì´í„° LDA
categorical_x = train_categorical.drop(
    ['Canonical_Smiles', 'Inhibition'], axis=1
)
categorical_test = test_categorical.drop(
    ['Canonical_Smiles'], axis=1
)
categorical_y = np.rint(train_categorical['Inhibition']).astype(int)

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda = LinearDiscriminantAnalysis(n_components=97)
categorical_x_lda = lda.fit_transform(categorical_x, categorical_y)
categorical_test_lda = lda.transform(categorical_test)

# evr = np.cumsum(lda.explained_variance_ratio_)
# num = [0.999, 0.99, 0.95, 0.90]
# for i in num :
#     threshold = i            
#     compo_1 = np.argmax(evr>= threshold) +1 
#     print(compo_1) 
# 0.999     99
# 0.99      97
# 0.95      87
# 0.90      77

lda_train = pd.concat([
    train_categorical[['Canonical_Smiles']].reset_index(drop=True),
    pd.DataFrame(categorical_x_lda, columns=[f'PC_{i+1}' for i in range(categorical_x_lda.shape[1])]),
    train_categorical[['Inhibition']].reset_index(drop=True)
], axis=1)

lda_test = pd.concat([
    test_categorical[['Canonical_Smiles']].reset_index(drop=True),
    pd.DataFrame(categorical_test_lda, columns=[f'PC_{i+1}' for i in range(categorical_test_lda.shape[1])])
], axis=1)

# print(lda_train.shape)     #  (1681, 99)
# print(pca_train.shape)     #  (1681, 37)
# print(lda_test.shape)      #  (100, 98)
# print(pca_test.shape)      #  (100, 36)

lda_train.to_csv('./Drug/_03/1_data/train_category.csv')
lda_test.to_csv('./Drug/_03/1_data/test_category.csv')
pca_train.to_csv('./Drug/_03/1_data/train_numeric.csv')
pca_test.to_csv('./Drug/_03/1_data/test_numeric.csv')

print(f'ì €ì¥ì™„ë£Œ! ë°ì´í„° ì œì‘ ëœë¤ê°’ì€ [{r}] ì´ì•¼!')


exit()
lda_x = lda_train.drop(['Canonical_Smiles','Inhibition'], axis=1)
lda_y = lda_train['Inhibition']
pca_x = pca_train.drop(['Canonical_Smiles','Inhibition'], axis=1)
pca_y = pca_train['Inhibition']

lda_y = np.rint(lda_y).astype(int)

lda_x_train, lda_x_test, lda_y_train, lda_y_test = train_test_split(
    lda_x, lda_y, random_state=r
)
pca_x_train, pca_x_test, pca_y_train, pca_y_test = train_test_split(
    pca_x, pca_y, random_state=r
)


