import os
import random
import dgl
import torch
import torch.nn as nn
import torch.nn.functional as F
from dgl.nn import GraphConv
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from datetime import datetime
import joblib


# âœ… ì‹œë“œ ê³ ì •
r = 73
random.seed(r)
np.random.seed(r)
torch.manual_seed(r)
if torch.cuda.is_available():
    torch.cuda.manual_seed(r)
    torch.cuda.manual_seed_all(r)
dgl.random.seed(r)


# =============================
# âœ… Dataset í´ë˜ìŠ¤
# =============================
class GraphDataset(Dataset):
    def __init__(self, graph_path, csv_path):
        self.graphs, _ = dgl.load_graphs(graph_path)
        csv = pd.read_csv(csv_path)

        self.graphs = [dgl.add_self_loop(g) for g in self.graphs]
        self.labels = torch.tensor(csv['Inhibition'].values, dtype=torch.float32)

    def __len__(self):
        return len(self.graphs)

    def __getitem__(self, idx):
        return self.graphs[idx], self.labels[idx]


# =============================
# âœ… collate í•¨ìˆ˜
# =============================
def collate(batch):
    graphs, labels = map(list, zip(*batch))
    batched_graph = dgl.batch(graphs)
    labels = torch.tensor(labels, dtype=torch.float32)
    return batched_graph, labels


# =============================
# âœ… GCN ëª¨ë¸
# =============================
class GCNRegressor(nn.Module):
    def __init__(self, in_feats, hidden_feats=128, dropout=0.3):
        super(GCNRegressor, self).__init__()
        self.conv1 = GraphConv(in_feats, hidden_feats, allow_zero_in_degree=True)
        self.conv2 = GraphConv(hidden_feats, hidden_feats, allow_zero_in_degree=True)
        self.conv3 = GraphConv(hidden_feats, hidden_feats, allow_zero_in_degree=True)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_feats, 1)

    def forward(self, g):
        h = g.ndata['h']
        h = F.relu(self.conv1(g, h))
        h = self.dropout(h)
        h = F.relu(self.conv2(g, h))
        h = self.dropout(h)
        h = F.relu(self.conv3(g, h))
        g.ndata['h_final'] = h
        hg = dgl.mean_nodes(g, 'h_final')
        return self.fc(hg)


# =============================
# âœ… NRMSE í•¨ìˆ˜
# =============================
def compute_nrmse(y_true, y_pred):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    return rmse / (y_true.max() - y_true.min())


# =============================
# âœ… ë°ì´í„° ë¡œë“œ
# =============================
train_graph_dataset = GraphDataset('./Drug/_engineered_data/train_graph.bin', './Drug/_engineered_data/filled_train_final.csv')
test_graph_dataset = GraphDataset('./Drug/_engineered_data/test_graph.bin', './Drug/_engineered_data/test_final.csv')

train_tabular = pd.read_csv('./Drug/_engineered_data/filled_train_final.csv')
test_tabular = pd.read_csv('./Drug/_engineered_data/test_final.csv')

X = train_tabular.drop(columns=['index', 'SMILES', 'Inhibition'], errors='ignore')
y = train_tabular['Inhibition'].values

y_max = y.max()
y_min = y.min()

X_test = test_tabular.drop(columns=['index', 'SMILES', 'Inhibition'], errors='ignore')

# âœ… ì €ì¥ í´ë”
os.makedirs('./Drug/_submission_files', exist_ok=True)
os.makedirs('./Drug/_models', exist_ok=True)


# =============================
# âœ… KFold ì‹œì‘
# =============================
kf = KFold(n_splits=5, shuffle=True, random_state=r)
fold = 1

gnn_nrmse_list = []
dnn_nrmse_list = []
ensemble_nrmse_list = []

gnn_test_fold_preds = []
dnn_test_fold_preds = []

for train_idx, val_idx in kf.split(X):
    print(f"\nğŸš€ Fold {fold} ì‹œì‘")

    # âœ… Graph Dataset Split
    train_graph = torch.utils.data.Subset(train_graph_dataset, train_idx)
    val_graph = torch.utils.data.Subset(train_graph_dataset, val_idx)

    train_loader = DataLoader(train_graph, batch_size=16, shuffle=True, collate_fn=collate)
    val_loader = DataLoader(val_graph, batch_size=16, shuffle=False, collate_fn=collate)

    # âœ… Tabular Dataset Split
    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)

    # âœ… GNN ëª¨ë¸
    in_feats = train_graph_dataset[0][0].ndata['h'].shape[1]
    gnn_model = GCNRegressor(in_feats).to('cpu')

    optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.001)
    loss_fn = nn.MSELoss()

    for epoch in range(50):
        gnn_model.train()
        total_loss = 0
        for batch, labels in train_loader:
            batch = batch.to('cpu')
            labels = labels.unsqueeze(1)

            preds = gnn_model(batch)
            loss = loss_fn(preds, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch+1} | GNN Train Loss: {avg_loss:.4f}")

    # âœ… GNN ê²€ì¦
    gnn_model.eval()
    gnn_preds_list = []
    val_labels_list = []

    with torch.no_grad():
        for batch, labels in val_loader:
            batch = batch.to('cpu')
            labels = labels.unsqueeze(1)

            preds = gnn_model(batch)

            gnn_preds_list.append(preds.numpy().flatten())
            val_labels_list.append(labels.numpy().flatten())

    gnn_preds = np.concatenate(gnn_preds_list)
    val_labels = np.concatenate(val_labels_list)

    gnn_nrmse = compute_nrmse(val_labels, gnn_preds)
    gnn_nrmse_list.append(gnn_nrmse)

    print(f"âœ… GNN Fold {fold} NRMSE: {gnn_nrmse:.4f}")

    # âœ… DNN ëª¨ë¸
    dnn_model = RandomForestRegressor(n_estimators=500, random_state=r)
    dnn_model.fit(X_train_scaled, y_train)

    dnn_preds = dnn_model.predict(X_val_scaled)

    dnn_nrmse = compute_nrmse(val_labels, dnn_preds)
    dnn_nrmse_list.append(dnn_nrmse)

    print(f"âœ… DNN Fold {fold} NRMSE: {dnn_nrmse:.4f}")

    # âœ… ì•™ìƒë¸”
    alpha = 0.7
    beta = 0.3
    ensemble_preds = alpha * gnn_preds + beta * dnn_preds

    ensemble_nrmse = compute_nrmse(val_labels, ensemble_preds)
    ensemble_nrmse_list.append(ensemble_nrmse)

    print(f"âœ… Ensemble Fold {fold} NRMSE: {ensemble_nrmse:.4f}")

    # âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
    test_loader = DataLoader(test_graph_dataset, batch_size=16, shuffle=False, collate_fn=collate)

    gnn_model.eval()
    gnn_test_preds = []
    with torch.no_grad():
        for batch, _ in test_loader:
            batch = batch.to('cpu')
            preds = gnn_model(batch)
            gnn_test_preds.append(preds.numpy().flatten())

    gnn_test_preds = np.concatenate(gnn_test_preds)
    gnn_test_fold_preds.append(gnn_test_preds)

    X_test_scaled = scaler.transform(X_test)
    dnn_test_preds = dnn_model.predict(X_test_scaled)
    dnn_test_fold_preds.append(dnn_test_preds)

    # âœ… ëª¨ë¸ ì €ì¥
    torch.save(gnn_model.state_dict(), f'./Drug/_models/gnn_model_fold{fold}.pt')
    joblib.dump(dnn_model, f'./Drug/_models/dnn_model_fold{fold}.pkl')
    joblib.dump(scaler, f'./Drug/_models/scaler_fold{fold}.pkl')

    fold += 1


# =============================
# âœ… í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì•™ìƒë¸”
# =============================
gnn_test_final = np.mean(gnn_test_fold_preds, axis=0)
dnn_test_final = np.mean(dnn_test_fold_preds, axis=0)

ensemble_test_preds = (alpha * gnn_test_final) + (beta * dnn_test_final)


# =============================
# âœ… ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
# =============================
print("\n============================")
print(f"âœ… GNN NRMSE: {gnn_nrmse_list}")
print(f"âœ… DNN NRMSE: {dnn_nrmse_list}")
print(f"âœ… Ensemble NRMSE: {ensemble_nrmse_list}")
print(f"âœ… í‰ê·  GNN NRMSE: {np.mean(gnn_nrmse_list):.4f}")
print(f"âœ… í‰ê·  DNN NRMSE: {np.mean(dnn_nrmse_list):.4f}")
print(f"âœ… í‰ê·  Ensemble NRMSE: {np.mean(ensemble_nrmse_list):.4f}")
print(f"âœ… Random State: {r}")
print("============================")


# âœ… CSV ì €ì¥
now = datetime.now().strftime('%Y%m%d_%H%M')
result_path = f'./Drug/_submission_files/ensemble_result_{now}.csv'

result_df = pd.DataFrame({
    'Fold': list(range(1, 6)),
    'GNN_NRMSE': gnn_nrmse_list,
    'DNN_NRMSE': dnn_nrmse_list,
    'Ensemble_NRMSE': ensemble_nrmse_list
})
result_df.loc['Mean'] = ['Mean', np.mean(gnn_nrmse_list), np.mean(dnn_nrmse_list), np.mean(ensemble_nrmse_list)]

result_df.to_csv(result_path, index=False)
print(f"\nâœ”ï¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {result_path}")


# âœ… Submission ì €ì¥
submission_path = f'./Drug/_submission_files/submission_ensemble_{now}.csv'
sub_csv = pd.read_csv('./Drug/sample_submission.csv')
sub_csv['Inhibition'] = ensemble_test_preds
sub_csv.to_csv(submission_path, index=False)

print(f"\nâœ”ï¸ ì•™ìƒë¸” ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ â†’ {submission_path}")
