import numpy as np
from sklearn.metrics import mean_squared_error
from scipy.stats import pearsonr
import pandas as pd

# ğŸ¯ ê³ ì • ì‹œë“œ
r = 394
np.random.seed(r)

# âœ… ë‘ ëª¨ë¸ì˜ OOF ë° ì˜ˆì¸¡ ê²°ê³¼ (ì˜ˆì‹œë¡œ ë¶ˆëŸ¬ì˜¨ë‹¤ê³  ê°€ì •)
gnn_oof = np.load("./Drug/_02/full_pipeline/gat_oof.npy")
gnn_pred = np.load("./Drug/_02/full_pipeline/gat_preds.npy")
pycaret_oof = np.load("./Drug/_02/full_pipeline/pycaret_oof.npy")
pycaret_pred = np.load("./Drug/_02/full_pipeline/pycaret_preds.npy")

# âœ… ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì„¤ì • (ê°€ì¤‘ í‰ê· , ê°€ì¤‘ì¹˜ëŠ” ì‹¤í—˜ì ìœ¼ë¡œ ì¡°ì •)
w1 = 0.6  # GNN
w2 = 0.4  # PyCaret

# âœ… ì•™ìƒë¸” ê²°ê³¼ ê³„ì‚°
final_oof = w1 * gnn_oof + w2 * pycaret_oof
final_pred = w1 * gnn_pred + w2 * pycaret_pred

# âœ… í‰ê°€ í•¨ìˆ˜ ì •ì˜
def evaluate(y_true, y_pred):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    nrmse = rmse / (np.max(y_true) - np.min(y_true))
    pearson = pearsonr(y_true, y_pred)[0]
    score = 0.5 * (1 - min(nrmse, 1)) + 0.5 * np.clip(pearson, 0, 1)
    return rmse, nrmse, pearson, score

# âœ… ìµœì¢… í‰ê°€
submission_df = pd.read_csv("./Drug/_02/full_pipeline/0_dataset/train_descriptor.csv")
y_true = submission_df["Inhibition"].values  # ì‹¤ì œê°’ ë¶ˆëŸ¬ì˜¤ê¸°
rmse, nrmse, pearson, score = evaluate(y_true, final_oof)

print(f"ğŸ“Š GNN + PyCaret ì•™ìƒë¸” ì„±ëŠ¥:")
print(f"RMSE     : {rmse:.5f}")
print(f"NRMSE    : {nrmse:.5f}")
print(f"Pearson  : {pearson:.5f}")
print(f"Score    : {score:.5f}")

# âœ… Submission ì €ì¥
import pandas as pd
submission = pd.read_csv("./Drug/sample_submission.csv")
submission["Inhibition"] = final_pred
submission.to_csv("submission_gnn_pycaret.csv", index=False)
