# ê¸°ë³¸
import os
import random
import datetime
import joblib
import warnings

# ì—°ì‚°
import numpy as np
import pandas as pd

# PyTorch
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# DGL
import dgl
from dgl.nn.pytorch import GATConv

# RDKit
from rdkit import Chem, RDLogger
from rdkit.Chem import Descriptors, AllChem, rdchem
from rdkit.Chem import GetPeriodicTable
from rdkit.ML.Descriptors import MoleculeDescriptors

# Sklearn
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold

# í†µê³„
from scipy.stats import pearsonr

# ëª¨ë¸
from lightgbm import LGBMRegressor, early_stopping, log_evaluation
from xgboost import XGBRegressor
from catboost import CatBoostRegressor

# PyCaret
from pycaret.regression import *

RDLogger.DisableLog('rdApp.*') 
warnings.filterwarnings('ignore')

# ì‹œë“œ ê³ ì •
r = random.randint(1,1000)
random.seed(r)
np.random.seed(r)
torch.manual_seed(r)

# ê²½ë¡œ
train_path = "./Drug/train.csv"
test_path = "./Drug/test.csv"
save_dir = "./Drug/_02/full_pipeline/0_dataset"
os.makedirs(save_dir, exist_ok=True)

# ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv(train_path)
test_df = pd.read_csv(test_path)

# SMILES ì¶”ì¶œ
train_smiles = train_df["Canonical_Smiles"].tolist()
test_smiles = test_df["Canonical_Smiles"].tolist()

# descriptor + fingerprint ì¶”ì¶œ í•¨ìˆ˜
def get_descriptor_fingerprint(smiles_list):
    descriptor_names = [desc[0] for desc in Descriptors._descList]
    calculator = MoleculeDescriptors.MolecularDescriptorCalculator(descriptor_names)

    all_features = []
    valid_indices = []

    for idx, smi in enumerate(smiles_list):
        mol = Chem.MolFromSmiles(smi)
        if mol is None:
            continue

        # RDKit Descriptors
        desc = list(calculator.CalcDescriptors(mol))

        # Morgan Fingerprint (bit vector)
        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)
        fp_array = np.zeros((2048,), dtype=int)
        AllChem.DataStructs.ConvertToNumpyArray(fp, fp_array)
        fp_bits = fp_array.tolist()

        # ê²°í•©
        features = desc + fp_bits
        all_features.append(features)
        valid_indices.append(idx)

    desc_cols = descriptor_names
    fp_cols = [f"FP_{i}" for i in range(2048)]
    all_cols = desc_cols + fp_cols

    return pd.DataFrame(all_features, columns=all_cols), valid_indices

# í”¼ì²˜ ìƒì„±
train_feat_df, train_valid_idx = get_descriptor_fingerprint(train_smiles)
test_feat_df, test_valid_idx = get_descriptor_fingerprint(test_smiles)

# index ë§ì¶”ê¸°
train_df = train_df.iloc[train_valid_idx].reset_index(drop=True)
test_df = test_df.iloc[test_valid_idx].reset_index(drop=True)

# Inhibition ì¶”ê°€
train_feat_df["Inhibition"] = train_df["Inhibition"].values

# 1. ìƒìˆ˜ì—´ ì œê±°
stds = train_feat_df.drop(columns="Inhibition").std()
non_constant_cols = stds[stds > 0].index.tolist()
train_feat_df = train_feat_df[non_constant_cols + ["Inhibition"]]
test_feat_df = test_feat_df[non_constant_cols]

# 2. ê²°ì¸¡ê°’ KNN ëŒ€ì²´
imputer = KNNImputer(n_neighbors=5)
X_all = pd.concat([train_feat_df.drop(columns="Inhibition"), test_feat_df])
X_imputed = imputer.fit_transform(X_all)
X_train = X_imputed[:len(train_feat_df)]
X_test = X_imputed[len(train_feat_df):]

# 3. ìƒê´€ê´€ê³„ ë†’ì€ í”¼ì²˜ ì œê±°
X_df = pd.DataFrame(X_train, columns=non_constant_cols)
corr_matrix = X_df.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]
X_df = X_df.drop(columns=to_drop)
X_test_df = pd.DataFrame(X_test, columns=non_constant_cols).drop(columns=to_drop)

# 4. í´ë¦¬í•‘
lower = X_df.quantile(0.01)
upper = X_df.quantile(0.99)
X_df = X_df.clip(lower=lower, upper=upper, axis=1)
X_test_df = X_test_df.clip(lower=lower, upper=upper, axis=1)
from sklearn.preprocessing import RobustScaler
# 5. ìŠ¤ì¼€ì¼ë§
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_df)
X_test_scaled = scaler.transform(X_test_df)

#==================================================================
from sklearn.decomposition import PCA

# # ëˆ„ì  ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨ ê¸°ì¤€ìœ¼ë¡œ ì°¨ì› ì„ íƒ
# pca = PCA(n_components=X_train_scaled.shape[0])
# pca.fit(X_train_scaled)

# # ëˆ„ì  ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨ ê³„ì‚°
# cumsum = np.cumsum(pca.explained_variance_ratio_)
# threshold = 0.999  # 96% ì´ìƒ ì„¤ëª…í•˜ë„ë¡
# n_components = np.argmax(cumsum >= threshold) + 1
# print(f"ğŸ¯ ì„ íƒëœ PCA ì°¨ì› ìˆ˜: {n_components}")

# exit()

# pca = PCA(n_components=1006)
# X_train_pca = pca.fit_transform(X_train_scaled)
# X_test_pca = pca.transform(X_test_scaled)

# 6. ìµœì¢… ì €ì¥
final_train = pd.DataFrame(X_train_scaled, )
final_test = pd.DataFrame(X_test_scaled,)
final_train["Inhibition"] = train_df["Inhibition"].values

#==================================================================

final_train.to_csv(f"{save_dir}/train_descriptor.csv", index=False)
final_test.to_csv(f"{save_dir}/test_descriptor.csv", index=False)

print("âœ… ì™„ë£Œ: ì „ì²´ ì—´ ìˆ˜ =", final_train.shape[1])

# ===================================================
# ì „ì²˜ë¦¬ ë°”ê¿ˆ
# ===================================================



# ê²½ë¡œ ì„¤ì •
train_path = "./Drug/train.csv"
test_path = "./Drug/test.csv"
save_dir = "./Drug/_02/full_pipeline/0_dataset"
os.makedirs(save_dir, exist_ok=True)

# ê³µìœ  ë°˜ì§€ë¦„ (Ã…) + ì „ê¸°ìŒì„±ë„ (Pauling scale)
COVALENT_RADII = {
    1: 0.31, 6: 0.76, 7: 0.71, 8: 0.66, 9: 0.57,
    15: 1.07, 16: 1.05, 17: 1.02, 35: 1.20, 53: 1.39
}
ELECTRONEGATIVITY = {
    1: 2.20, 6: 2.55, 7: 3.04, 8: 3.44, 9: 3.98,
    15: 2.19, 16: 2.58, 17: 3.16, 35: 2.96, 53: 2.66
}

##### 1. ë°ì´í„° ë¡œë”© #####
train_df = pd.read_csv(train_path)
test_df = pd.read_csv(test_path)

##### 2. Mol ë³€í™˜ í•¨ìˆ˜ (ì˜ˆì™¸ ì²˜ë¦¬ í¬í•¨) #####
def smiles_to_mol(smiles):
    try:
        mol = Chem.MolFromSmiles(smiles)
        if mol is not None:
            Chem.SanitizeMol(mol)
        return mol
    except:
        return None

train_df['mol'] = train_df['Canonical_Smiles'].apply(smiles_to_mol)
test_df['mol'] = test_df['Canonical_Smiles'].apply(smiles_to_mol)

# ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥
train_df[train_df['mol'].isnull()].to_csv(f"{save_dir}/invalid_train_mol.csv", index=False)
test_df[test_df['mol'].isnull()].to_csv(f"{save_dir}/invalid_test_mol.csv", index=False)

##### 3. í”¼ì²˜ ì •ì˜ #####
ptable = GetPeriodicTable()

# ê³µìœ  ë°˜ì§€ë¦„ ë§¤í•‘
COVALENT_RADII = {
    1: 0.31, 6: 0.76, 7: 0.71, 8: 0.66, 9: 0.57,
    15: 1.07, 16: 1.05, 17: 1.02, 35: 1.20, 53: 1.39
}

# one-hot ìœ ì—°í™”
def one_hot_encoding(value, choices):
    return [int(value == c) for c in choices]

# ì›ì í”¼ì²˜ (ì•ˆì •í˜•)
def atom_features(atom):
    atomic_num_choices = [1, 6, 7, 8, 9, 15, 16, 17, 35, 53]
    hybridization_choices = [
        rdchem.HybridizationType.SP,
        rdchem.HybridizationType.SP2,
        rdchem.HybridizationType.SP3,
        rdchem.HybridizationType.SP3D,
        rdchem.HybridizationType.SP3D2,
    ]

    atomic_num = atom.GetAtomicNum()
    electronegativity = ELECTRONEGATIVITY.get(atomic_num, 0.0)
    covalent_radius = COVALENT_RADII.get(atomic_num, 1.0)

    features = []
    features += one_hot_encoding(atomic_num, atomic_num_choices)
    features.append(atom.GetDegree())
    features.append(atom.GetFormalCharge())
    features.append(atom.GetTotalNumHs())
    features += one_hot_encoding(atom.GetHybridization(), hybridization_choices)
    features.append(int(atom.GetIsAromatic()))
    features.append(atom.GetImplicitValence())
    features.append(atom.GetTotalValence()) 
    features.append(atom.GetNumRadicalElectrons())
    features.append(int(atom.IsInRing()))
    features.append(covalent_radius)
    features.append(electronegativity)

    return torch.tensor(features, dtype=torch.float)


# ê²°í•© í”¼ì²˜
def bond_features(bond):
    stereo_choices = list(rdchem.BondStereo.values)
    stereo_encoded = one_hot_encoding(bond.GetStereo(), stereo_choices)
    return torch.tensor([
        bond.GetBondTypeAsDouble(),
        int(bond.GetIsConjugated()),
        int(bond.IsInRing()),
        *stereo_encoded
    ], dtype=torch.float)

##### 4. Mol â†’ DGLGraph ë³€í™˜ #####
def mol_to_dgl_graph(mol):
    num_atoms = mol.GetNumAtoms()
    atom_feats = [atom_features(mol.GetAtomWithIdx(i)) for i in range(num_atoms)]
    
    srcs, dsts, bond_feats = [], [], []
    for bond in mol.GetBonds():
        u, v = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()
        srcs += [u, v]
        dsts += [v, u]
        b_feat = bond_features(bond)
        bond_feats += [b_feat, b_feat]

    # ê·¸ë˜í”„ ìƒì„± (ì›ë˜ ì—£ì§€ ê¸°ì¤€)
    g = dgl.graph((srcs, dsts), num_nodes=num_atoms)

    # âœ… self-loop ì¶”ê°€
    g = dgl.add_self_loop(g)

    # âœ… self-loopì— ëŒ€ì‘í•˜ëŠ” dummy bond feature ì¶”ê°€
    num_edges = g.num_edges()
    num_bond_feats = len(bond_feats)

    if num_edges > num_bond_feats:
        # í•˜ë‚˜ì˜ dummy bond featureëŠ” ê¸°ì¡´ ê²ƒê³¼ ê°™ì€ shapeì„ ê°€ì ¸ì•¼ í•¨
        dummy_bond = torch.zeros_like(bond_feats[0])
        for _ in range(num_edges - num_bond_feats):
            bond_feats.append(dummy_bond)

    # ë…¸ë“œ ë° ì—£ì§€ í”¼ì²˜ ì €ì¥
    g.ndata['h'] = torch.stack(atom_feats)
    g.edata['e'] = torch.stack(bond_feats)

    return g

##### 5. ìˆœì°¨ ë³€í™˜ ë° ì €ì¥ #####
train_graphs = [mol_to_dgl_graph(mol) for mol in train_df['mol'] if mol is not None]
test_graphs = [mol_to_dgl_graph(mol) for mol in test_df['mol'] if mol is not None]

joblib.dump(train_graphs, f"{save_dir}/train_graphs.pkl")
joblib.dump(test_graphs, f"{save_dir}/test_graphs.pkl")

##### 6. ë¶„ì ì „ì²´ íŠ¹ì„± ì €ì¥ (ì„ íƒ) #####
train_df['MolWt'] = train_df['mol'].apply(lambda m: Descriptors.MolWt(m) if m else np.nan)
train_df['logP'] = train_df['mol'].apply(lambda m: Descriptors.MolLogP(m) if m else np.nan)
train_df[['Canonical_Smiles', 'MolWt', 'logP']].to_csv(f"{save_dir}/train_mol_features.csv", index=False)

print("âœ… GNN ê·¸ë˜í”„ ë³€í™˜ ë° ì €ì¥ ì™„ë£Œ!")


##### GAT ëª¨ë¸ êµ¬ì„± #####


class GraphDataset(Dataset):
    def __init__(self, graphs, labels=None):
        self.graphs = graphs
        self.labels = labels

    def __len__(self):
        return len(self.graphs)

    def __getitem__(self, idx):
        g = self.graphs[idx]
        if self.labels is not None:
            label = torch.tensor(self.labels[idx], dtype=torch.float32)
            return g, label
        else:
            return g

from dgl.nn import Set2Set, GATv2Conv

class GATv2Set2Set(nn.Module):
    def __init__(self, in_node_feats, hidden_size=128, num_heads=4, dropout=0.2):
        super().__init__()
        self.gat1 = GATv2Conv(in_node_feats, hidden_size, num_heads)
        self.gat2 = GATv2Conv(hidden_size * num_heads, hidden_size, 1)
        self.set2set = Set2Set(hidden_size, n_iters=6, n_layers=1)
        self.readout = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(2 * hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 1)
        )

    def forward(self, g):
        h = g.ndata['h']
        h = self.gat1(g, h).flatten(1)
        h = F.elu(h)
        h = self.gat2(g, h).mean(1)
        g.ndata['h'] = h
        hg = self.set2set(g, h)
        return self.readout(hg).squeeze(-1)


def collate_graphs(batch):
    graphs, labels = zip(*batch)
    return dgl.batch(graphs), torch.stack(labels)

def evaluate(model, dataloader, device):
    model.eval()
    preds = []
    with torch.no_grad():
        for batch in dataloader:
            g = batch.to(device)
            pred = model(g)
            preds.append(pred.cpu().numpy())
    return np.concatenate(preds)

def main():
    train_graphs = joblib.load("./Drug/_02/full_pipeline/0_dataset/train_graphs.pkl")
    test_graphs = joblib.load("./Drug/_02/full_pipeline/0_dataset/test_graphs.pkl")
    train_df = pd.read_csv("./Drug/train.csv")
    test_df = pd.read_csv("./Drug/test.csv")
    y = train_df["Inhibition"].values

    # âœ… CPU ê³ ì •
    device = torch.device("cpu")

    n_splits = 5
    oof = np.zeros(len(train_graphs))
    test_preds_each_fold = []

    kf = KFold(n_splits=n_splits, shuffle=True, random_state=r)
    for fold, (tr_idx, val_idx) in enumerate(kf.split(train_graphs)):
        print(f"ğŸ” Fold {fold + 1}")

        tr_graphs = [train_graphs[i] for i in tr_idx]
        val_graphs = [train_graphs[i] for i in val_idx]
        y_tr, y_val = y[tr_idx], y[val_idx]

        train_dataset = GraphDataset(tr_graphs, y_tr)
        val_dataset = GraphDataset(val_graphs, y_val)
        test_dataset = GraphDataset(test_graphs)

        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_graphs)
        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_graphs)
        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=lambda x: dgl.batch(x))

        in_node_feats = train_graphs[0].ndata['h'].shape[1]

        model = GATv2Set2Set(in_node_feats=in_node_feats, hidden_size=128, num_heads=4, dropout=0.2).to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
        criterion = nn.MSELoss()

        best_rmse = float('inf')
        patience = 10
        patience_counter = 0

        for epoch in range(1, 101):
            model.train()
            for batch in train_loader:
                g, yb = batch
                g = g.to(device)
                yb = yb.to(device)
                pred = model(g)
                loss = criterion(pred, yb)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            model.eval()
            val_preds = []
            with torch.no_grad():
                for batch in val_loader:
                    g, _ = batch
                    g = g.to(device)
                    pred = model(g).squeeze(-1)
                    val_preds.append(np.atleast_1d(pred.cpu().numpy()))
            val_preds = np.concatenate(val_preds) 
            # print(f"val_preds.shape = {val_preds.shape}, y_val.shape = {y_val.shape}")
            val_rmse = np.sqrt(mean_squared_error(y_val, val_preds))

            print(f"Epoch {epoch:03d} | Val RMSE: {val_rmse:.4f}")

            if val_rmse < best_rmse:
                best_rmse = val_rmse
                patience_counter = 0
                torch.save(model.state_dict(), f"./Drug/_02/full_pipeline/gat_fold{fold}.pt")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print("â¹ Early stopping triggered")
                    break

        model.load_state_dict(torch.load(f"./Drug/_02/full_pipeline/gat_fold{fold}.pt"))
        model.eval()
        val_preds = []
        with torch.no_grad():
            for batch in val_loader:
                g, _ = batch
                g = g.to(device)
                pred = model(g)
                val_preds.append(pred.cpu().numpy())
        oof[val_idx] = np.concatenate(val_preds)

        test_pred = evaluate(model, test_loader, device)
        test_preds_each_fold.append(test_pred)

    test_preds = np.mean(test_preds_each_fold, axis=0)
    test_std = np.std(test_preds_each_fold, axis=0)
    print(f"ğŸ“Š Test prediction std across folds: {np.mean(test_std):.5f}")

    np.save("./Drug/_02/full_pipeline/gat_oof.npy", oof)
    np.save("./Drug/_02/full_pipeline/gat_preds.npy", test_preds)

    submission = pd.DataFrame({
        "ID": test_df["ID"],
        "Inhibition": test_preds
    })
    
    now = datetime.datetime.now().strftime("%Y%m%d_%H%M")
    filename = f"pre_GAT_{r}_({now}).csv"
    save_path = f"./Drug/_02/full_pipeline/{filename}"
    
    submission.to_csv(save_path, index=False)
    print("âœ… gat_oof.npy, gat_preds.npy, pre_GAT.csv ì €ì¥ ì™„ë£Œ")


if __name__ == "__main__":
    main()
    


def print_scores(y_true, y_pred, label=""):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    nrmse = rmse / (np.max(y_true) - np.min(y_true))
    pearson = pearsonr(y_true, y_pred)[0]
    score = 0.5 * (1 - min(nrmse, 1)) + 0.5 * np.clip(pearson, 0, 1)
    print(f"ğŸ“Š {label} ê²°ê³¼")
    print(f"RMSE     : {rmse:.5f}")
    print(f"NRMSE    : {nrmse:.5f}")
    print(f"Pearson  : {pearson:.5f}")
    print(f"ScoreğŸ“ˆ  : {score:.5f}")

train = pd.read_csv("./Drug/_02/full_pipeline/0_dataset/train_descriptor.csv")
test = pd.read_csv("./Drug/_02/full_pipeline/0_dataset/test_descriptor.csv")
X = train.drop(columns=["Inhibition"])
y = train["Inhibition"].values
X_test = test.copy()

kf = KFold(n_splits=5, shuffle=True, random_state=r)

param_lgbm = {
    "n_estimators": 1000,
    "learning_rate": 0.015875869112729403,
    "max_depth": 5,
    "num_leaves": 77,
    "min_child_samples": 21,
    "colsample_bytree": 0.8956019941712514,
    "random_state": r
}

param_cat = {
    "iterations": 1000,
    "learning_rate": 0.020782688572110002,
    "depth": 4,
    "l2_leaf_reg": 9.322000582507451,
    "verbose": 0,
    "random_seed": r
}

param_xgb = {
    "n_estimators": 1000,
    "learning_rate": 0.010607554597937956,
    "max_depth": 3,
    "subsample": 0.7961002707424782,
    "colsample_bytree": 0.8105132963922017,
    "verbosity": 0,
    "random_state": r
}

models = {
    "lgbm": LGBMRegressor(**param_lgbm, verbosity=-1),
    "xgb": XGBRegressor(**param_xgb, early_stopping_rounds=20),
    "cat": CatBoostRegressor(**param_cat)
}

oof_preds = {name: np.zeros(len(X)) for name in models}
test_preds = {name: np.zeros(len(X_test)) for name in models}

print("ğŸ”¹ Boosting ëª¨ë¸ë³„ 5-Fold í•™ìŠµ ì‹œì‘")

for name, model in models.items():
    print(f"âœ… {name.upper()} ì‹œì‘")
    for train_idx, val_idx in kf.split(X):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        if name == "lgbm":
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                eval_metric="rmse",
                callbacks=[early_stopping(stopping_rounds=20), log_evaluation(period=0)],
            )
        elif name == "xgb":
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                verbose=False
            )
        elif name == "cat":
            model.fit(
                X_train, y_train,
                eval_set=(X_val, y_val),
                early_stopping_rounds=20,
                use_best_model=True
            )

        oof_preds[name][val_idx] = model.predict(X_val)
        test_preds[name] += model.predict(X_test) / kf.n_splits

    print_scores(y, oof_preds[name], label=f"{name.upper()}")

# OOF stacking ì•™ìƒë¸”
print("ğŸ”¹ OOF ì•™ìƒë¸” ìµœì  alpha íƒìƒ‰")
best_score = -np.inf
best_weights = None
alphas = np.linspace(0, 1, 11)

for a in alphas:
    for b in alphas:
        if a + b > 1: continue
        c = 1 - a - b
        blended = a * oof_preds["lgbm"] + b * oof_preds["xgb"] + c * oof_preds["cat"]
        rmse = np.sqrt(mean_squared_error(y, blended))
        nrmse = rmse / (np.max(y) - np.min(y))
        pearson = pearsonr(y, blended)[0]
        score = 0.5 * (1 - min(nrmse, 1)) + 0.5 * np.clip(pearson, 0, 1)
        if score > best_score:
            best_score = score
            best_weights = (a, b, c)

a, b, c = best_weights
print(f"âœ… ìµœì  ê°€ì¤‘ì¹˜: LGBM={a:.2f}, XGB={b:.2f}, CAT={c:.2f} | Score: {best_score:.5f}")

final_oof = a * oof_preds["lgbm"] + b * oof_preds["xgb"] + c * oof_preds["cat"]
final_test = a * test_preds["lgbm"] + b * test_preds["xgb"] + c * test_preds["cat"]

print_scores(y, final_oof, label="Stacked Ensemble")

submission = pd.DataFrame({
    "ID": pd.read_csv("./Drug/test.csv")["ID"],
    "Inhibition": final_test
})

import datetime

# í˜„ì¬ ì‹œê°„ í¬ë§·
now = datetime.datetime.now().strftime("%Y%m%d_%H%M")

# íŒŒì¼ëª… ìƒì„±
filename = f"pre_DNN_{r}_({now}).csv"
save_path = f"./Drug/_02/full_pipeline/{filename}"

# ì €ì¥
submission.to_csv(save_path, index=False)
print(f"âœ… {filename} ì €ì¥ ì™„ë£Œ")

np.save("./Drug/_02/full_pipeline/boost_oof.npy", final_oof)
np.save("./Drug/_02/full_pipeline/boost_preds.npy", final_test)
print("âœ… boost_oof.npy, boost_preds.npy ì €ì¥ ì™„ë£Œ")

# ===== File: 05_ensemble.py ===== #


# ì ìˆ˜ ì¶œë ¥ í•¨ìˆ˜
def print_scores(y_true, y_pred, label=""):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    nrmse = rmse / (np.max(y_true) - np.min(y_true))
    pearson = pearsonr(y_true, y_pred)[0]
    score = 0.5 * (1 - min(nrmse, 1)) + 0.5 * np.clip(pearson, 0, 1)
    print(f"ğŸ“Š {label} ê²°ê³¼")
    print(f"RMSE     : {rmse:.5f}")
    print(f"NRMSE    : {nrmse:.5f}")
    print(f"Pearson  : {pearson:.5f}")
    print(f"ScoreğŸ“ˆ  : {score:.5f}")
    return score

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
y = pd.read_csv("./Drug/train.csv")["Inhibition"].values
dmpnn_oof = np.load("./Drug/_02/full_pipeline/gat_oof.npy")
boost_oof = np.load("./Drug/_02/full_pipeline/boost_oof.npy")
dmpnn_preds = np.load("./Drug/_02/full_pipeline/gat_preds.npy")
boost_preds = np.load("./Drug/_02/full_pipeline/boost_preds.npy")
test_id = pd.read_csv("./Drug/test.csv")["ID"]

# ì•™ìƒë¸” ìµœì  alpha ì°¾ê¸°
alphas = np.linspace(0, 1, 21)
best_score = -np.inf
best_alpha = 0.5

for alpha in alphas:
    final_oof = alpha * dmpnn_oof + (1 - alpha) * boost_oof
    score = print_scores(y, final_oof, label=f"Î±={alpha:.2f}")
    if score > best_score:
        best_score = score
        best_alpha = alpha

# ìµœì¢… ì•™ìƒë¸” ì˜ˆì¸¡
final_preds = best_alpha * dmpnn_preds + (1 - best_alpha) * boost_preds
final_oof = best_alpha * dmpnn_oof + (1 - best_alpha) * boost_oof

# ì ìˆ˜ ì¶œë ¥
print_scores(y, final_oof, label=f"Final Ensemble Î±={best_alpha:.2f}")

# ì €ì¥
now = datetime.datetime.now().strftime("%Y%m%d_%H%M")
filename = f"submission_{r}_final_ensemble({now}).csv"
save_path = f"./Drug/_02/full_pipeline/{filename}"

submission = pd.DataFrame({
    "ID": test_id,
    "Inhibition": final_preds
})
submission.to_csv(save_path, index=False)
print(f"âœ… ìµœì¢… ì•™ìƒë¸” íŒŒì¼ ì €ì¥ ì™„ë£Œ â†’ {filename}")
print('random_state :', r)


train = pd.read_csv("./Drug/_02/full_pipeline/0_dataset/train_descriptor.csv")
test = pd.read_csv("./Drug/_02/full_pipeline/0_dataset/test_descriptor.csv")

oof_preds = np.zeros(len(train))
test_preds = []

kf = KFold(n_splits=5, shuffle=True, random_state=r)

for fold, (train_idx, valid_idx) in enumerate(kf.split(train)):
    fold_train = train.iloc[train_idx].copy()
    fold_valid = train.iloc[valid_idx].copy()

    setup(data=fold_train, target='Inhibition', session_id=r,
          normalize=True, verbose=False, use_gpu=False, )

    model = compare_models(include=['lr', 'ridge', 'lasso','rf', 'gbr'], sort='R2')
    final_model = finalize_model(model)

    valid_preds = predict_model(final_model, data=fold_valid)
    
    # âœ… ì˜ˆì¸¡ ì»¬ëŸ¼ëª… ìë™ ê°ì§€
    label_col = None
    for col in ['Label', 'prediction_label', 'Prediction']:
        if col in valid_preds.columns:
            label_col = col
            break
    if label_col is None:
        label_col = valid_preds.columns[-1]

    oof_preds[valid_idx] = valid_preds[label_col].values

    test_pred_df = predict_model(final_model, data=test)
    test_preds.append(test_pred_df[label_col].values if label_col in test_pred_df.columns else test_pred_df.iloc[:, -1].values)

# í‰ê·  test ì˜ˆì¸¡
test_preds_mean = np.mean(test_preds, axis=0)

np.save("./Drug/_02/full_pipeline/pycaret_oof.npy", oof_preds)
np.save("./Drug/_02/full_pipeline/pycaret_preds.npy", test_preds_mean)

def print_scores(y_true, y_pred, label=""):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    nrmse = rmse / (np.max(y_true) - np.min(y_true))
    pearson = pearsonr(y_true, y_pred)[0]
    score = 0.5 * (1 - min(nrmse, 1)) + 0.5 * np.clip(pearson, 0, 1)
    print(f"ğŸ“Š {label}")
    print(f"RMSE     : {rmse:.5f}")
    print(f"NRMSE    : {nrmse:.5f}")
    print(f"Pearson  : {pearson:.5f}")
    print(f"ScoreğŸ“ˆ  : {score:.5f}")
    return score

# ë°ì´í„° ë¡œë“œ
y = pd.read_csv("./Drug/train.csv")["Inhibition"].values
gat_oof = np.load("./Drug/_02/full_pipeline/gat_oof.npy")
boost_oof = np.load("./Drug/_02/full_pipeline/boost_oof.npy")
pycaret_oof = np.load("./Drug/_02/full_pipeline/pycaret_oof.npy")       # âœ… ì¶”ê°€
gat_preds = np.load("./Drug/_02/full_pipeline/gat_preds.npy")
boost_preds = np.load("./Drug/_02/full_pipeline/boost_preds.npy")
pycaret_preds = np.load("./Drug/_02/full_pipeline/pycaret_preds.npy")   # âœ… test ì˜ˆì¸¡
test_id = pd.read_csv("./Drug/test.csv")["ID"]

#=================
# ìµœì  ê°€ì¤‘ì¹˜ ì°¾ê¸°
#=================

best_score = -np.inf
best_weights = (0.3, 0.3, 0.4)  # ì´ˆê¸°ê°’

# a: GAT, b: Boost, c: PyCaret
for a in np.linspace(0, 1, 11):
    for b in np.linspace(0, 1 - a, 11):
        c = 1.0 - a - b
        if c < 0 or c > 1:
            continue
        
        # OOF ê¸°ì¤€ í‰ê°€
        blend_oof = a * gat_oof + b * boost_oof + c * pycaret_oof
        score = print_scores(y, blend_oof, label=f"Î±={a:.2f}, Î²={b:.2f}, Î³={c:.2f}")
        
        if score > best_score:
            best_score = score
            best_weights = (a, b, c)

# ìµœì  ê°€ì¤‘ì¹˜ ì¶”ì¶œ
a, b, c = best_weights
# a, b, c = 0.05, 0.05, 0.90
print(f"âœ… ìµœì  ê°€ì¤‘ì¹˜: Î±(GAT)={a:.2f}, Î²(Boost)={b:.2f}, Î³(PyCaret)={c:.2f}, Score={best_score:.5f}")

# test ì˜ˆì¸¡
inter_blend = a * gat_preds + b * boost_preds + c * pycaret_preds
final_oof = a * gat_oof + b * boost_oof + c * pycaret_oof

# ì ìˆ˜ ì¶œë ¥
final_score = print_scores(y, final_oof, label=f"Final Ensemble Î±={a:.2f}, Î²={b:.2f}, Î³={c:.2f}")

# ì €ì¥
submission = pd.DataFrame({
    "ID": test_id,
    "Inhibition": inter_blend
})
now = datetime.datetime.now().strftime("%Y%m%d_%H%M")
filename = f"submission_{r}_final_ensemble_opt_weight({now}).csv"
submission.to_csv(f"./Drug/_02/full_pipeline/{filename}", index=False)
print(f"ëœë¤ ì‹œë“œê°’ : {r}")
print(f"âœ… ìµœì¢… ì €ì¥: {filename}")

import shutil

# ğŸ¯ ê¸°ì¤€ ì ìˆ˜ ì„¤ì •
SCORE_THRESHOLD = 0.61616  # ì›í•˜ëŠ” ê¸°ì¤€ê°’ìœ¼ë¡œ ì„¤ì •

# ì ìˆ˜ê°€ ê¸°ì¤€ë³´ë‹¤ ë‚®ìœ¼ë©´ ì „ì²´ ë””ë ‰í† ë¦¬ ì‚­ì œ
if final_score < SCORE_THRESHOLD:
    shutil.rmtree("./Drug/_02/full_pipeline")
    print(f"ğŸš« Score {final_score:.5f} < ê¸°ì¤€ {SCORE_THRESHOLD} â†’ ì „ì²´ ë””ë ‰í† ë¦¬ ì‚­ì œ ì™„ë£Œ")
else:
    print(f"ğŸ‰ Score {final_score:.5f} â‰¥ ê¸°ì¤€ {SCORE_THRESHOLD} â†’ ë””ë ‰í† ë¦¬ ìœ ì§€")

# âœ… ìµœì  ê°€ì¤‘ì¹˜: Î±(GAT)=0.20, Î²(Boost)=0.32, Î³(PyCaret)=0.48, Score=0.56802
# ğŸ“Š Final Ensemble Î±=0.20, Î²=0.32, Î³=0.48
# RMSE     : 24.62871
# NRMSE    : 0.24782
# Pearson  : 0.38386
# ScoreğŸ“ˆ  : 0.56802
# ëœë¤ ì‹œë“œê°’ : 707

# ğŸ“Š Final Ensemble Î±=0.20, Î²=0.24, Î³=0.56
# RMSE     : 24.46559
# NRMSE    : 0.24618
# Pearson  : 0.40069
# ScoreğŸ“ˆ  : 0.57726
# ëœë¤ ì‹œë“œê°’ : 916

# ğŸ“Š Final Ensemble Î±=0.30, Î²=0.14, Î³=0.56
# RMSE     : 24.46458
# NRMSE    : 0.24617
# Pearson  : 0.40795
# ScoreğŸ“ˆ  : 0.58089
# ëœë¤ ì‹œë“œê°’ : 830

# âœ… ìµœì  ê°€ì¤‘ì¹˜: Î±(GAT)=0.00, Î²(Boost)=0.50, Î³(PyCaret)=0.50, Score=0.61351
# ğŸ“Š Final Ensemble Î±=0.00, Î²=0.50, Î³=0.50
# RMSE     : 23.46277
# NRMSE    : 0.23609
# Pearson  : 0.46311
# ScoreğŸ“ˆ  : 0.61351
# ëœë¤ ì‹œë“œê°’ : 549

# âœ… ìµœì  ê°€ì¤‘ì¹˜: Î±(GAT)=0.00, Î²(Boost)=0.60, Î³(PyCaret)=0.40, Score=0.61616
# ğŸ“Š Final Ensemble Î±=0.00, Î²=0.60, Î³=0.40
# RMSE     : 23.42074
# NRMSE    : 0.23566
# Pearson  : 0.46798
# ScoreğŸ“ˆ  : 0.61616
# ëœë¤ ì‹œë“œê°’ : 368